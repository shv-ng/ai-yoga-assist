# AI Yoga Assist

This project aims to provide real-time yoga pose recognition and correction using Computer Vision and Deep Learning.

---

## Project Workflow: Steps to Achieve the Goal

### Step 1: Data Collection

**Goal:** Build a CSV dataset of pose sequences recorded via webcam.

* **How it works:** A Python script opens the webcam, allowing the user to press `R` to start recording and `S` to stop. Every frame, MediaPipe extracts 33 landmarks ( for each, totaling 99 numbers). Each frame is saved as a row.
* **What to record:** Six poses: *Vrikshasana, Utkatasana, Virabhadrasana, Bhujangasana, Adho Mukha Svanasana, and Utkata Konasana*. Record the full motion (neutral stance → move into pose → hold 2-3s → return to neutral).
* **Quantity:** 50–80 samples per pose across different lighting conditions and participants.
* **What to avoid:** Do not record static holds or intentional "wrong" versions.
* **Output:** One CSV file with columns: `frame_id`, `sequence_id`, `label`, `x0, y0, z0 ... x32, y32, z32`.
* **Tools:** Python, MediaPipe, OpenCV.

---

### Step 2: Pose Classification Model (CNN/Dense Network)

**Goal:** A model that identifies which of the six poses a person is attempting in a single frame.

* **Rationale:** The system must identify the pose before it can offer specific corrections (e.g., you can't give Warrior Pose feedback to someone in Tree Pose).
* **How it works:** Each individual frame (99 landmark coordinates) acts as a training sample.
* **Architecture:** * Input (99 neurons)  Dense (128)  Dropout  Dense (64)  Dropout  Output (6 classes with Softmax).
* **Thresholding:** If the highest confidence score is below 0.6, the output is labeled "Unknown Pose."
* **Output:** A trained classification model file (`.h5` or `.tflite`) with a target accuracy of ~90%.
* **Tools:** Google Colab, TensorFlow/Keras.

---

### Step 3: Pose Correction Model (BiLSTM)

**Goal:** A model that monitors a sliding window of the last 30 frames to judge if the motion trajectory is correct.

* **Why BiLSTM:** It possesses "memory" of movement. It understands the correct path into a pose and can catch improper rotation mid-motion.
* **How it works:** Sequences are grouped by `sequence_id` into a 3D input shape (frames, 99 landmarks).
* **Architecture:** * Input (30 frames)  BiLSTM (64 units)  Dropout (0.3)  BiLSTM (32 units)  Dense (16)  Output (Correction categories).
* **Data Augmentation:** "Incorrect" labels are generated by artificially perturbing landmark coordinates (shifting landmarks, flipping axes) from the "correct" recorded sequences.
* **Output:** Trained BiLSTM model and a Python dictionary mapping categories to feedback strings (e.g., "Raise your right arm higher").
* **Tools:** Google Colab, TensorFlow/Keras.

---

### Step 4: Real-Time Integration

**Goal:** A single Python script that integrates both models for live webcam use.

* **Pipeline Flow:** 1.  Webcam frame input  MediaPipe landmark extraction.
2.  Classification model identifies the attempted pose.
3.  Frame is added to a 30-frame rolling buffer.
4.  BiLSTM analyzes the buffer and outputs correction flags.
5.  OpenCV overlays feedback on the video feed.
* **User Interface:** Displays the MediaPipe skeleton, the detected pose name, and color-coded feedback (Red for corrections, Green for "Good Form").
* **UI Logic:** Correction text is set to persist for 2–3 seconds via a timer to prevent flickering.
* **Tools:** Python, OpenCV, MediaPipe, TensorFlow.

---

### Step 5: Testing, Polish, and Report

**Goal:** Validate system performance and document findings.

* **Testing:** Calculate accuracy and generate a **Confusion Matrix** to visualize pose misclassifications.
* **Polishing:** Implement edge-case handling (e.g., "Please adjust position" if landmarks disappear or lighting is insufficient).
* **Report Content:** * System architecture diagrams.
* Performance graphs (Accuracy/Loss over epochs).
* Comparative table against existing literature.
* Demo video of the live system.


* **Future Scope:** Raspberry Pi deployment, voice feedback (`pyttsx3`), mobile app integration, and expanded pose library.
* **Tools:** Scikit-learn, Matplotlib, Camcorder/Phone for demo.
